{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 6, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 60, "column": 0}, "map": {"version":3,"sources":["file://C%3A/Users/OscarStaniszewski/Downloads/llm-boilerplate-master/llm-boilerplate-master/src/ollama/tools/examMaster.ts"],"sourcesContent":["import { ToolFunction } from \"../toolsLoader\";\n\nconst functions: ToolFunction[] = [\n  {\n    type: \"function\",\n    function: {\n      name: \"examMaster\",\n      description: \"Generates a school exam based on subject, difficulty level, and number of questions.\",\n      parameters: {\n        type: \"object\",\n        properties: {\n          subject: {\n            type: \"string\",\n            description: \"The school subject (e.g. Math, History, Biology)\",\n          },\n          level: {\n            type: \"string\",\n            enum: [\"easy\", \"medium\", \"hard\"],\n            description: \"The difficulty level of the exam\",\n          },\n        },\n        required: [\"subject\", \"level\", \"questionsCount\"],\n      },\n    },\n    execute: async ({ subject, level, questionsCount }) => {\n      // Fejkowy generator pytaÅ„\n      const exam = Array.from({ length: questionsCount }, (_, i) => ({\n        question: `(${i + 1}) [${level}] Question about ${subject}?`,\n        options: [\"A\", \"B\", \"C\", \"D\"],\n        answer: \"A\",\n      }));\n\n      return {\n        subject,\n        level,\n        questions: exam,\n      };\n    },\n  },\n];\n\nexport default functions;\n"],"names":[],"mappings":";;;AAEA,MAAM,YAA4B;IAChC;QACE,MAAM;QACN,UAAU;YACR,MAAM;YACN,aAAa;YACb,YAAY;gBACV,MAAM;gBACN,YAAY;oBACV,SAAS;wBACP,MAAM;wBACN,aAAa;oBACf;oBACA,OAAO;wBACL,MAAM;wBACN,MAAM;4BAAC;4BAAQ;4BAAU;yBAAO;wBAChC,aAAa;oBACf;gBACF;gBACA,UAAU;oBAAC;oBAAW;oBAAS;iBAAiB;YAClD;QACF;QACA,SAAS,OAAO,EAAE,OAAO,EAAE,KAAK,EAAE,cAAc,EAAE;YAChD,0BAA0B;YAC1B,MAAM,OAAO,MAAM,IAAI,CAAC;gBAAE,QAAQ;YAAe,GAAG,CAAC,GAAG,IAAM,CAAC;oBAC7D,UAAU,CAAC,CAAC,EAAE,IAAI,EAAE,GAAG,EAAE,MAAM,iBAAiB,EAAE,QAAQ,CAAC,CAAC;oBAC5D,SAAS;wBAAC;wBAAK;wBAAK;wBAAK;qBAAI;oBAC7B,QAAQ;gBACV,CAAC;YAED,OAAO;gBACL;gBACA;gBACA,WAAW;YACb;QACF;IACF;CACD;uCAEc","debugId":null}},
    {"offset": {"line": 122, "column": 0}, "map": {"version":3,"sources":["file://C%3A/Users/OscarStaniszewski/Downloads/llm-boilerplate-master/llm-boilerplate-master/src/ollama/tools/index.ts"],"sourcesContent":["import examMaster from \"./examMaster\";\n\nconst ollamaTools = [...examMaster];\n\nexport default ollamaTools;\n"],"names":[],"mappings":";;;AAAA;;AAEA,MAAM,cAAc;OAAI,sIAAA,CAAA,UAAU;CAAC;uCAEpB","debugId":null}},
    {"offset": {"line": 137, "column": 0}, "map": {"version":3,"sources":["file://C%3A/Users/OscarStaniszewski/Downloads/llm-boilerplate-master/llm-boilerplate-master/src/ollama/toolsLoader.ts"],"sourcesContent":["import { Tool } from \"ollama\";\nimport ollamaTools from \"./tools\";\n\nexport interface ToolFunction extends Tool {\n  execute: (...args: any[]) => Promise<any>;\n}\n\nexport function getTools() {\n  return ollamaTools.filter((item) => \"execute\" in item) as ToolFunction[];\n}\n\nexport { ollamaTools };\n"],"names":[],"mappings":";;;AACA;;AAMO,SAAS;IACd,OAAO,iIAAA,CAAA,UAAW,CAAC,MAAM,CAAC,CAAC,OAAS,aAAa;AACnD","debugId":null}},
    {"offset": {"line": 177, "column": 0}, "map": {"version":3,"sources":["file://C%3A/Users/OscarStaniszewski/Downloads/llm-boilerplate-master/llm-boilerplate-master/src/services/ollama.ts"],"sourcesContent":["import { getTools } from \"@/ollama/toolsLoader\";\nimport { Ollama, Message } from \"ollama\";\n\nconst OLLAMA_HOST = process.env.OLLAMA_HOST || \"http://localhost:11434\";\nconst OLLAMA_MODEL = process.env.OLLAMA_MODEL || \"qwen3:14b\";\nconst TOOLS_RECURSION_LIMIT = 10;\nconst SYSTEM_PROMPT = `\n  You are a helpful AI assistant.\n  \n  Ask yourself \"What tools can I use?\".\n\n  Use markdown formatting for better readability.\n\n  IMPORTANT: Do not simulate tool call, use them directly.\n  Do not send full tool response, make it short and concise.\n\n  If asked about current weather, use the getWeather tool, use the getLocation tool for longitude and latitude.\n` as const;\n\nconst ollamaClient = new Ollama({\n  host: OLLAMA_HOST,\n});\n\ninterface ToolCall {\n  id: string;\n  type: \"function\";\n  function: {\n    name: string;\n    arguments: string;\n  };\n}\n\nasync function handleToolCalls(\n  toolCalls: unknown[]\n): Promise<Record<string, unknown>> {\n  const tools = getTools();\n  const results: Record<string, unknown> = {};\n\n  for (const call of toolCalls as ToolCall[]) {\n    const tool = tools.find((t) => t.function.name === call.function.name);\n    if (!tool) {\n      results[call.function.name] = { error: \"Tool not found\" };\n      continue;\n    }\n\n    try {\n      const args = call.function.arguments;\n      const result = await tool.execute(args);\n      results[call.function.name] = result;\n    } catch (error) {\n      results[call.function.name] = {\n        error: error instanceof Error ? error.message : String(error),\n      };\n    }\n  }\n\n  return results;\n}\nexport async function generateResponse(\n  message: string,\n  history: Message[] = [],\n  recursionCount: number = 0\n): Promise<{ messages: Message[] }> {\n  if (recursionCount > TOOLS_RECURSION_LIMIT) {\n    return {\n      messages: [\n        {\n          role: \"assistant\",\n          content:\n            \"I'm having trouble processing your request. The tool calls are taking too long.\",\n        },\n      ],\n    };\n  }\n\n  try {\n    const messages: Message[] = [\n      { role: \"system\", content: SYSTEM_PROMPT },\n      ...history,\n    ];\n\n    if (recursionCount === 0) {\n      messages.push({ role: \"user\", content: message });\n    }\n\n    const response = await ollamaClient.chat({\n      model: OLLAMA_MODEL,\n      messages,\n      stream: false,\n      tools: getTools(),\n    });\n\n    // if there are no tool calls, return the response content directly\n    if (!response.message?.tool_calls) {\n      return {\n        messages: [\n          {\n            role: \"assistant\",\n            content:\n              response.message?.content ||\n              \"I'm having trouble processing your request. Please try again.\",\n          },\n        ],\n      };\n    }\n\n    const toolResults = await handleToolCalls(response.message.tool_calls);\n\n    // Add tool responses to message history\n    const toolMessages: Message[] = [];\n    for (const result of Object.values(toolResults)) {\n      toolMessages.push({\n        role: \"tool\",\n        content: JSON.stringify(result),\n      });\n    }\n\n    const nextResponse = await generateResponse(\n      message,\n      [...messages, ...toolMessages],\n      recursionCount + 1\n    );\n    return {\n      messages: [...toolMessages, ...nextResponse.messages],\n    };\n  } catch (error) {\n    console.error(\"Error calling Ollama:\", error);\n    throw error;\n  }\n}\n"],"names":[],"mappings":";;;AAAA;AAAA;AACA;;;AAEA,MAAM,cAAc,QAAQ,GAAG,CAAC,WAAW,IAAI;AAC/C,MAAM,eAAe,QAAQ,GAAG,CAAC,YAAY,IAAI;AACjD,MAAM,wBAAwB;AAC9B,MAAM,gBAAgB,CAAC;;;;;;;;;;;AAWvB,CAAC;AAED,MAAM,eAAe,IAAI,0IAAA,CAAA,SAAM,CAAC;IAC9B,MAAM;AACR;AAWA,eAAe,gBACb,SAAoB;IAEpB,MAAM,QAAQ,CAAA,GAAA,8IAAA,CAAA,WAAQ,AAAD;IACrB,MAAM,UAAmC,CAAC;IAE1C,KAAK,MAAM,QAAQ,UAAyB;QAC1C,MAAM,OAAO,MAAM,IAAI,CAAC,CAAC,IAAM,EAAE,QAAQ,CAAC,IAAI,KAAK,KAAK,QAAQ,CAAC,IAAI;QACrE,IAAI,CAAC,MAAM;YACT,OAAO,CAAC,KAAK,QAAQ,CAAC,IAAI,CAAC,GAAG;gBAAE,OAAO;YAAiB;YACxD;QACF;QAEA,IAAI;YACF,MAAM,OAAO,KAAK,QAAQ,CAAC,SAAS;YACpC,MAAM,SAAS,MAAM,KAAK,OAAO,CAAC;YAClC,OAAO,CAAC,KAAK,QAAQ,CAAC,IAAI,CAAC,GAAG;QAChC,EAAE,OAAO,OAAO;YACd,OAAO,CAAC,KAAK,QAAQ,CAAC,IAAI,CAAC,GAAG;gBAC5B,OAAO,iBAAiB,QAAQ,MAAM,OAAO,GAAG,OAAO;YACzD;QACF;IACF;IAEA,OAAO;AACT;AACO,eAAe,iBACpB,OAAe,EACf,UAAqB,EAAE,EACvB,iBAAyB,CAAC;IAE1B,IAAI,iBAAiB,uBAAuB;QAC1C,OAAO;YACL,UAAU;gBACR;oBACE,MAAM;oBACN,SACE;gBACJ;aACD;QACH;IACF;IAEA,IAAI;QACF,MAAM,WAAsB;YAC1B;gBAAE,MAAM;gBAAU,SAAS;YAAc;eACtC;SACJ;QAED,IAAI,mBAAmB,GAAG;YACxB,SAAS,IAAI,CAAC;gBAAE,MAAM;gBAAQ,SAAS;YAAQ;QACjD;QAEA,MAAM,WAAW,MAAM,aAAa,IAAI,CAAC;YACvC,OAAO;YACP;YACA,QAAQ;YACR,OAAO,CAAA,GAAA,8IAAA,CAAA,WAAQ,AAAD;QAChB;QAEA,mEAAmE;QACnE,IAAI,CAAC,SAAS,OAAO,EAAE,YAAY;YACjC,OAAO;gBACL,UAAU;oBACR;wBACE,MAAM;wBACN,SACE,SAAS,OAAO,EAAE,WAClB;oBACJ;iBACD;YACH;QACF;QAEA,MAAM,cAAc,MAAM,gBAAgB,SAAS,OAAO,CAAC,UAAU;QAErE,wCAAwC;QACxC,MAAM,eAA0B,EAAE;QAClC,KAAK,MAAM,UAAU,OAAO,MAAM,CAAC,aAAc;YAC/C,aAAa,IAAI,CAAC;gBAChB,MAAM;gBACN,SAAS,KAAK,SAAS,CAAC;YAC1B;QACF;QAEA,MAAM,eAAe,MAAM,iBACzB,SACA;eAAI;eAAa;SAAa,EAC9B,iBAAiB;QAEnB,OAAO;YACL,UAAU;mBAAI;mBAAiB,aAAa,QAAQ;aAAC;QACvD;IACF,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,yBAAyB;QACvC,MAAM;IACR;AACF","debugId":null}},
    {"offset": {"line": 298, "column": 0}, "map": {"version":3,"sources":["file://C%3A/Users/OscarStaniszewski/Downloads/llm-boilerplate-master/llm-boilerplate-master/src/app/api/chat/route.ts"],"sourcesContent":["import { NextResponse } from \"next/server\";\nimport { generateResponse } from \"@/services/ollama\";\n\nexport async function POST(request: Request) {\n  try {\n    const { message, history } = await request.json();\n\n    if (!message) {\n      return NextResponse.json(\n        { error: \"Message is required\" },\n        { status: 400 }\n      );\n    }\n\n    const response = await generateResponse(message, history);\n    return NextResponse.json({ response });\n  } catch (error) {\n    console.error(\"Error in chat API:\", error);\n    return NextResponse.json(\n      { error: \"Failed to process request\" },\n      { status: 500 }\n    );\n  }\n}\n"],"names":[],"mappings":";;;AAAA;AACA;;;AAEO,eAAe,KAAK,OAAgB;IACzC,IAAI;QACF,MAAM,EAAE,OAAO,EAAE,OAAO,EAAE,GAAG,MAAM,QAAQ,IAAI;QAE/C,IAAI,CAAC,SAAS;YACZ,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CACtB;gBAAE,OAAO;YAAsB,GAC/B;gBAAE,QAAQ;YAAI;QAElB;QAEA,MAAM,WAAW,MAAM,CAAA,GAAA,2HAAA,CAAA,mBAAgB,AAAD,EAAE,SAAS;QACjD,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CAAC;YAAE;QAAS;IACtC,EAAE,OAAO,OAAO;QACd,QAAQ,KAAK,CAAC,sBAAsB;QACpC,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CACtB;YAAE,OAAO;QAA4B,GACrC;YAAE,QAAQ;QAAI;IAElB;AACF","debugId":null}}]
}